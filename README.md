{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"README.md","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0P7h6s++qZ8+Hro0QEiBm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Reviews analysis block for online reputation management system (ORM)**\n","\n","The block allows you to evaluate sentiment analysis and summarize the text\n","\n","## **Data**\n","### **Getting data**\n","\n","Data (reviews, names of banks, rating) are obtained from the site sravni.ru using parsing in 3 stages:\n","\n","1. Parsing pages like https://www.sravni.ru/banki/otzyvy/?page=N, where N is the page number for parsing. We parse pages to obtain data in the format: id, link, bank, rating. Parsing is carried out in parts. We connect separate files and get the final dataset **list.csv**\n","\n","2. Parsing reviews from pages like https://www.sravni.ru/bank/B/otzyvy/ID, where B is the bank, ID is the review id. Links are taken from a previously generated dataset. We parse pages to get data in the format: 'id', 'text', 'bank', 'rating'. Parsing is carried out in parts.\n","\n","3. We connect separate files and get the final dataset **data.csv**\n","\n","Result: **list.csv**, **data.csv**\n","\n","### **Data analysis**\n","\n","We check your data for gaps, look at statistics and balance classes. Save a balanced dataset **data_balanced**\n","\n","## **Models**\n","\n","### **Classification**\n","\n","To categorize reviews (by class: negative, neutral, positive), BERT fine tuning was used to perform a sentiment analysis of the dataset\n","\n","As a basis for creating the code, a guide was taken from the official TensorFlow website https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n","\n","* The preprocessing model: **bert_multi_cased_preprocess/3**\n","\n","Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text\n","\n","* BERT model: **bert_multi_cased_L-12_H-768_A-12/4**\n","\n","Use the model \"pooled_output\" after which add our layers for classification\n","\n","\n","### **Summarization**\n","\n","Summarization model\n","Take a pre-trained model **MBARTRuSumGazeta** = https://huggingface.co/IlyaGusev/mbart_ru_sum_gazeta\n","The model is pre-trained on \"Dataset for Automatic Summarization of Russian News\" - https://arxiv.org/abs/2006.11063\n","\n","The model showed good results on customer reviews\n","\n","## **Result**\n","\n","The result folder contains a report on the result of the work in PDF format"],"metadata":{"id":"uCTq1bv3I5bC"}}]}